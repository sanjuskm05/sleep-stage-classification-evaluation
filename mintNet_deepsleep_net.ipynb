{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mintNet_deepsleep_net.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1TawEn2kcjguZYQJEVTo0JPhHQGBiFiX1","authorship_tag":"ABX9TyMygLeAb+ChNfBbcG6JKLwG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"09915fe4aeef4eaa92d00f037305822e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7feae53f82964d8fbbbee8d7db258757","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3b82c968dc094e06b1158c6172f82eff","IPY_MODEL_ac388e2bc4f94facb2d60302e03cca2f"]}},"7feae53f82964d8fbbbee8d7db258757":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3b82c968dc094e06b1158c6172f82eff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_89e5485b3feb42b58efe21c59595bff5","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":150,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":150,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7d636caa7191449690e722fb9719a29d"}},"ac388e2bc4f94facb2d60302e03cca2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_67d1f099c0f24e7d82f32eb531cc15d2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 150/150 [01:47&lt;00:00,  1.40it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c4f953e45f2a4d078137bebaf7c6bec9"}},"89e5485b3feb42b58efe21c59595bff5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7d636caa7191449690e722fb9719a29d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"67d1f099c0f24e7d82f32eb531cc15d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c4f953e45f2a4d078137bebaf7c6bec9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e9113abfe1784ef58fb93cb2c3fa625b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b606df85aaaf41aca965d6f55cd000e6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c25a6e648f9b4826a168a7245c5bd485","IPY_MODEL_a69e533d3699490f8d89f260d05bdbac"]}},"b606df85aaaf41aca965d6f55cd000e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c25a6e648f9b4826a168a7245c5bd485":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_439efa5423d2461a89d2d32123532921","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":70,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":70,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ae6e81444bdf447eac83fe661402a486"}},"a69e533d3699490f8d89f260d05bdbac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5a547df129984adb97c9677d28871306","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 70/70 [03:04&lt;00:00,  2.63s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_273f243c216a44a3aab57358e265f3dc"}},"439efa5423d2461a89d2d32123532921":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ae6e81444bdf447eac83fe661402a486":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5a547df129984adb97c9677d28871306":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"273f243c216a44a3aab57358e265f3dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MTLTv53HbeIB","executionInfo":{"status":"ok","timestamp":1618885023960,"user_tz":240,"elapsed":7719,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}},"outputId":"9a7846d6-834a-4cfc-e26d-c0a9a47d5b33"},"source":["!pip install mne"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting mne\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/29/7f38c7c99ca65fe4aac9054239d885c44ab7f9e8b4f65e9f2bfa489b0f38/mne-0.22.1-py3-none-any.whl (6.9MB)\n","\u001b[K     |████████████████████████████████| 6.9MB 9.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from mne) (1.19.5)\n","Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from mne) (1.4.1)\n","Installing collected packages: mne\n","Successfully installed mne-0.22.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b1hdA4IKD0Nc","executionInfo":{"status":"ok","timestamp":1618885119710,"user_tz":240,"elapsed":590,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}}},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"koKn0O9qd5bS","executionInfo":{"status":"ok","timestamp":1618885127027,"user_tz":240,"elapsed":1254,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}}},"source":["#Source : https://github.com/akaraspt/deepsleepnet\n","import re, operator, logging\n","import numpy as np\n","from collections import namedtuple\n","import h5py\n","import random\n","from datetime import datetime\n","EVENT_CHANNEL = 'EDF Annotations'\n","log = logging.getLogger(__name__)\n","\n","class EDFEndOfData: pass\n","\n","WINDOW_SIZE = 100\n","\n","def rescale_array(X):\n","    X = X / 20\n","    X = np.clip(X, -5, 5)\n","    return X\n","\n","\n","def aug_X(X):\n","    scale = 1 + np.random.uniform(-0.1, 0.1)\n","    offset = np.random.uniform(-0.1, 0.1)\n","    noise = np.random.normal(scale=0.05, size=X.shape)\n","    X = scale * X + offset + noise\n","    return X\n","\n","def gen(dict_files, aug=False):\n","    while True:\n","        record_name = random.choice(list(dict_files.keys()))\n","        batch_data = dict_files[record_name]\n","        all_rows = batch_data['x']\n","\n","        for i in range(10):\n","            start_index = random.choice(range(all_rows.shape[0]-WINDOW_SIZE))\n","\n","            X = all_rows[start_index:start_index+WINDOW_SIZE, ...]\n","            Y = batch_data['y'][start_index:start_index+WINDOW_SIZE]\n","\n","            X = np.expand_dims(X, 0)\n","            Y = np.expand_dims(Y, -1)\n","            Y = np.expand_dims(Y, 0)\n","\n","            if aug:\n","                X = aug_X(X)\n","            X = rescale_array(X)\n","\n","            yield X, Y\n","\n","\n","def chunker(seq, size=WINDOW_SIZE):\n","    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n","\n","def tal(tal_str):\n","  exp = '(?P<onset>[+\\-]\\d+(?:\\.\\d*)?)' + \\\n","        '(?:\\x15(?P<duration>\\d+(?:\\.\\d*)?))?' + \\\n","        '(\\x14(?P<annotation>[^\\x00]*))?' + \\\n","        '(?:\\x14\\x00)'\n","\n","  def annotation_to_list(annotation):\n","    # print('annotatn:',annotation)\n","    return str(annotation).split('\\x14') if annotation else []\n","\n","  def parse(dic):\n","    return (\n","    float(dic['onset']), float(dic['duration']) if dic['duration'] else 0., annotation_to_list(dic['annotation']))\n","\n","  return [parse(m.groupdict()) for m in re.finditer(exp, tal_str)]\n","\n","def edf_header(f):\n","    h = {}\n","    assert f.tell() == 0  # check file position\n","    assert f.read(8) == '0       '\n","    # recording info)\n","    h['local_subject_id'] = f.read(80).strip()\n","    h['local_recording_id'] = f.read(80).strip()\n","    # parse timestamp\n","    (day, month, year) = [int(x) for x in re.findall('(\\d+)', f.read(8))]\n","    (hour, minute, sec) = [int(x) for x in re.findall('(\\d+)', f.read(8))]\n","    h['date_time'] = str(datetime(year + 2000, month, day, hour, minute, sec))\n","    # misc\n","    header_nbytes = int(f.read(8))\n","    subtype = f.read(44)[:5]\n","    h['EDF+'] = subtype in ['EDF+C', 'EDF+D']\n","    h['contiguous'] = subtype != 'EDF+D'\n","    h['n_records'] = int(f.read(8))\n","    h['record_length'] = float(f.read(8))  # in seconds\n","    nchannels = h['n_channels'] = int(f.read(4))\n","    # read channel info\n","    channels = range(h['n_channels'])\n","    h['label'] = [f.read(16).strip() for n in channels]\n","    h['transducer_type'] = [f.read(80).strip() for n in channels]\n","    h['units'] = [f.read(8).strip() for n in channels]\n","    h['physical_min'] = np.asarray([float(f.read(8)) for n in channels])\n","    h['physical_max'] = np.asarray([float(f.read(8)) for n in channels])\n","    h['digital_min'] = np.asarray([float(f.read(8)) for n in channels])\n","    h['digital_max'] = np.asarray([float(f.read(8)) for n in channels])\n","    h['prefiltering'] = [f.read(80).strip() for n in channels]\n","    h['n_samples_per_record'] = [int(f.read(8)) for n in channels]\n","    f.read(32 * nchannels)  # reserved\n","    assert f.tell() == header_nbytes\n","    return h\n","\n","class BaseEDFReader:\n","  def __init__(self, file, verbose=False):\n","    self.file = file\n","    self.verbose = verbose\n","\n","  def read_header(self):\n","    if self.verbose:\n","        print('read_header')\n","    self.header = h = edf_header(self.file)\n","    # calculate ranges for rescaling\n","    self.dig_min = h['digital_min']\n","    self.phys_min = h['physical_min']\n","    phys_range = h['physical_max'] - h['physical_min']\n","    dig_range = h['digital_max'] - h['digital_min']\n","    assert np.all(phys_range > 0)\n","    assert np.all(dig_range > 0)\n","    self.gain = phys_range / dig_range\n","\n","  def read_raw_record(self):\n","    if self.verbose:\n","        print('read raw record')\n","    # Read a record with data and return a list containing arrays with raw bytes.\n","    result = []\n","    for nsamp in self.header['n_samples_per_record']:\n","      samples = self.file.read(nsamp * 2)\n","      if len(samples) != nsamp * 2:\n","        raise EDFEndOfData\n","      result.append(samples)\n","    return result\n","\n","  def convert_record(self, raw_record):\n","    if self.verbose:\n","        print('convert record')\n","    # Convert a raw record to a (time, signals, events) tuple based on information in the header.\n","    h = self.header\n","    dig_min, phys_min, gain = self.dig_min, self.phys_min, self.gain\n","    time = float('nan')\n","    signals = []\n","    events = []\n","    for (i, samples) in enumerate(raw_record):\n","      if h['label'][i] == EVENT_CHANNEL:\n","        ann = tal(samples)\n","        time = ann[0][0]\n","        events.extend(ann[1:])\n","      # print(i, samples)\n","      # exit()\n","      else:\n","        # 2-byte little-endian integers\n","        dig = np.fromstring(samples, '<i2').astype(np.float32)\n","        phys = (dig - dig_min[i]) * gain[i] + phys_min[i]\n","        signals.append(phys)\n","    return time, signals, events\n","\n","  def read_record(self):\n","    if self.verbose:\n","        print('read_record')\n","    return self.convert_record(self.read_raw_record())\n","\n","  def records(self):\n","    if self.verbose:\n","        print('record self')\n","    # Record generator.\n","    yield self.read_record()\n","    \"\"\"try:\n","        while True:\n","            yield self.read_record()\n","    except EDFEndOfData:\n","        pass\"\"\"\n","\n","def load_edf(edffile):\n","  \"\"\"Load an EDF+ file.\n","   Very basic reader for EDF and EDF+ files. While BaseEDFReader does support\n","exotic features like non-homogeneous sample rates and loading only parts of\n","the stream, load_edf expects a single fixed sample rate for all channels and\n","tries to load the whole file.\n","Parameters\n","----------\n","edffile : file-like object or string\n","Returns\n","-------\n","Named tuple with the fields:\n","  X : NumPy array with shape p by n.\n","    Raw recording of n samples in p dimensions.\n","  sample_rate : float\n","    The sample rate of the recording. Note that mixed sample-rates are not\n","    supported.\n","  sens_lab : list of length p with strings\n","    The labels of the sensors used to record X.\n","  time : NumPy array with length n\n","    The time offset in the recording for each sample.\n","  annotations : a list with tuples      EDF+ annotations are stored in (start, duration, description) tuples.\n","    start : float\n","      Indicates the start of the event in seconds.\n","    duration : float\n","      Indicates the duration of the event in seconds.\n","    description : list with strings\n","      Contains (multiple?) descriptions of the annotation event.\"\"\"\n","  if isinstance(edffile, basestring):\n","    with open(edffile, 'rb') as f:\n","      return load_edf(f)  # convert filename to file\n","  reader = BaseEDFReader(edffile)\n","  reader.read_header()\n","  h = reader.header\n","  log.debug('EDF header: %s' % h)\n","  # get sample rate info\n","  nsamp = np.unique(\n","    [n for (l, n) in zip(h['label'], h['n_samples_per_record'])\n","     if l != EVENT_CHANNEL])\n","  assert nsamp.size == 1, 'Multiple sample rates not supported!'\n","  sample_rate = float(nsamp[0]) / h['record_length']\n","  rectime, X, annotations = zip(*reader.records())\n","  X = np.hstack(X)\n","  annotations = reduce(operator.add, annotations)\n","  chan_lab = [lab for lab in reader.header['label'] if lab != EVENT_CHANNEL]\n","  # create timestamps\n","  if reader.header['contiguous']:\n","    time = np.arange(X.shape[1]) / sample_rate\n","  else:\n","    reclen = reader.header['record_length']\n","    within_rec_time = np.linspace(0, reclen, nsamp, endpoint=False)\n","    time = np.hstack([t + within_rec_time for t in rectime])\n","  tup = namedtuple('EDF', 'X sample_rate chan_lab time annotations')\n","  return tup(X, sample_rate, chan_lab, time, annotations)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"eZUn9u4eiLzP","executionInfo":{"status":"ok","timestamp":1618885140480,"user_tz":240,"elapsed":3827,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}}},"source":["import ntpath\n","import random, os, sys\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import mne\n","from mne.datasets.sleep_physionet._utils import _fetch_one, _data_path, AGE_SLEEP_RECORDS, _check_subjects\n","from datetime import datetime"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"52KplZzjbkzc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618885231189,"user_tz":240,"elapsed":528,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}},"outputId":"f576bc18-2188-450c-c142-57fe36e74ea2"},"source":["W = 0\n","N1 = 1\n","N2 = 2\n","N3 = 3\n","REM = 4\n","UNKNOWN = 5\n","\n","label_dict = {\n","    \"W\": W,\n","    \"N1\": N1,\n","    \"N2\": N2,\n","    \"N3\": N3,\n","    \"REM\": REM,\n","    \"UNKNOWN\": UNKNOWN\n","}\n","\n","class_dict = {\n","    0: \"W\",\n","    1: \"N1\",\n","    2: \"N2\",\n","    3: \"N3\",\n","    4: \"REM\",\n","    5: \"UNKNOWN\"\n","}\n","\n","annot2label = {\n","    \"Sleep stage W\": 0,\n","    \"Sleep stage 1\": 1,\n","    \"Sleep stage 2\": 2,\n","    \"Sleep stage 3\": 3,\n","    \"Sleep stage 4\": 3,\n","    \"Sleep stage R\": 4,\n","    \"Sleep stage ?\": 5,\n","    \"Movement time\": 5\n","}\n","project_path = os.path.abspath(os.getcwd())+'/drive/MyDrive'\n","print(project_path)\n","\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S0fF6hB4jFmr","executionInfo":{"status":"ok","timestamp":1618885232750,"user_tz":240,"elapsed":543,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}}},"source":["data_path = _data_path\n","BASE_URL = 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette/'\n","def fetch_data(subjects, recording=[1, 2], path=None, force_update=False,\n","               update_path=None, base_url=BASE_URL,\n","               verbose=None):  # noqa: D301\n","    records = np.loadtxt(AGE_SLEEP_RECORDS,\n","                         skiprows=1,\n","                         delimiter=',',\n","                         usecols=(0, 1, 2, 6, 7),\n","                         dtype={'names': ('subject', 'record', 'type', 'sha',\n","                                          'fname'),\n","                                'formats': ('<i2', 'i1', '<S9', 'S40', '<S22')}\n","                         )\n","    psg_records = records[np.where(records['type'] == b'PSG')]\n","    hyp_records = records[np.where(records['type'] == b'Hypnogram')]\n","\n","    path = data_path(path=path, update_path=update_path)\n","    params = [path, force_update, base_url]\n","    fnames = []\n","    for subject in subjects:\n","        for idx in np.where(psg_records['subject'] == subject)[0]:\n","            if psg_records['record'][idx] in recording:\n","                psg_fname = _fetch_one(psg_records['fname'][idx].decode(),\n","                                       psg_records['sha'][idx].decode(),\n","                                       *params)\n","                hyp_fname = _fetch_one(hyp_records['fname'][idx].decode(),\n","                                       hyp_records['sha'][idx].decode(),\n","                                       *params)\n","                fnames.append([psg_fname, hyp_fname])\n","\n","    return fnames"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"OprhG6vAj686","executionInfo":{"status":"ok","timestamp":1618885235764,"user_tz":240,"elapsed":558,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}}},"source":["subjects_list = []  # list to keep the address of the subject data\n","except_sub = [13, 36, 52]  # omitting the subjects with incomplete data \n","for i in range(83):\n","    if i in except_sub:\n","        continue\n","    subjects_list.append(i)\n","# fetching data of each subject and \n","subject_files = fetch_data(subjects=subjects_list, recording=[1, 2], path= project_path) "],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"EhoqPhh8kEx0","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["09915fe4aeef4eaa92d00f037305822e","7feae53f82964d8fbbbee8d7db258757","3b82c968dc094e06b1158c6172f82eff","ac388e2bc4f94facb2d60302e03cca2f","89e5485b3feb42b58efe21c59595bff5","7d636caa7191449690e722fb9719a29d","67d1f099c0f24e7d82f32eb531cc15d2","c4f953e45f2a4d078137bebaf7c6bec9"]},"executionInfo":{"status":"ok","timestamp":1618885249204,"user_tz":240,"elapsed":982,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}},"outputId":"1e387c3e-7e7c-49f9-9dfc-7c6f1af7744e"},"source":["from tqdm.notebook import tqdm\n","from mne import Epochs, pick_types, find_events\n","from mne.io import concatenate_raws, read_raw_edf\n","from mne.time_frequency import psd_welch\n","\n","mapping = {'EOG horizontal': 'eog',\n","           'Resp oro-nasal': 'misc',\n","           'EMG submental': 'misc',\n","           'Temp rectal': 'misc',\n","           'Event marker': 'misc'}\n","VBS = True  # constant boolean to enable/disbale verbose\n","EPOCH_SEC_SIZE = 30  # Epoch duration selection\n","seed = 42  # seed value for the random seeds\n","number_of_subj = 70           \n","ch_labels =  ['EEG Fpz-Cz', 'EEG Pz-Oz']  # channels to be selected\n","\n","output_path = os.path.join(project_path, \"NPZ_files\")  # path to save the npz files\n","# loop to preprocess input data and save the results in npz files to be used in our models later\n","for item in tqdm(subject_files):\n","    filename = ntpath.basename(item[0]).replace(\"-PSG.edf\", \".npz\")  # reading the PSG files\n","    if not os.path.exists(os.path.join(output_path, filename)):\n","        raw_train = mne.io.read_raw_edf(item[0], verbose=VBS)\n","        sampling_rate = raw_train.info['sfreq']\n","        raw_ch_df = raw_train.to_data_frame(scalings=100.0)[ch_labels]\n","        #raw_ch_df = raw_ch_df.to_frame()\n","        raw_ch_df.set_index(np.arange(len(raw_ch_df)))\n","        \n","        # reading the raw headers using the EDFReader function from edfreader\n","        f = open(item[0], 'r', errors='ignore', encoding='utf-8')\n","        head_raw_read = BaseEDFReader(f)\n","        head_raw_read.read_header()\n","        head_raw = head_raw_read.header\n","        f.close()\n","        raw_start_time = datetime.strptime(head_raw['date_time'], \"%Y-%m-%d %H:%M:%S\")\n","\n","        # read annotations from hypnogram file\n","        f = open(item[1], 'r')\n","        annot_raw_read = BaseEDFReader(f)\n","        annot_raw_read.read_header()\n","        annot_raw = annot_raw_read.header\n","        temp, temp, total_annot = zip(*annot_raw_read.records())\n","        f.close()\n","        annot_start_time = datetime.strptime(annot_raw['date_time'], \"%Y-%m-%d %H:%M:%S\")\n","        assert raw_start_time == annot_start_time  # making sure that the PSG files and hypnogram files are in sync\n","        remove_idx = []    # list to keep the indicies of data that will be removed\n","        labels = []        # list to keep the indicies of data that have labels\n","        label_idx = []\n","        \n","        # selecting the indicies of known labels and adding the rest to remove_idx list\n","        for annot in total_annot[0]:\n","            onset_sec, duration_sec, annot_char = annot\n","            annot_str = \"\".join(annot_char)\n","            label = annot2label[annot_str]\n","            if label != UNKNOWN:\n","                if duration_sec % EPOCH_SEC_SIZE != 0:\n","                    raise Exception(\"Please choose anothe epoch duration!\")\n","                duration_epoch = int(duration_sec / EPOCH_SEC_SIZE)\n","                label_epoch = np.ones(duration_epoch, dtype=np.int) * label\n","                labels.append(label_epoch)\n","                idx = int(onset_sec * sampling_rate) + np.arange(duration_sec * sampling_rate, dtype=np.int)\n","                label_idx.append(idx)\n","            else:\n","                idx = int(onset_sec * sampling_rate) + np.arange(duration_sec * sampling_rate, dtype=np.int)\n","                remove_idx.append(idx)\n","        labels = np.hstack(labels)\n","        if len(remove_idx) > 0:\n","            remove_idx = np.hstack(remove_idx)\n","            select_idx = np.setdiff1d(np.arange(len(raw_ch_df)), remove_idx)\n","        else:\n","            select_idx = np.arange(len(raw_ch_df))\n","\n","        # filtering data with labels only\n","        label_idx = np.hstack(label_idx)\n","        select_idx = np.intersect1d(select_idx, label_idx)\n","\n","        # removing extra indicies\n","        if len(label_idx) > len(select_idx):\n","            extra_idx = np.setdiff1d(label_idx, select_idx)\n","            # trimming the tail\n","            if np.all(extra_idx > select_idx[-1]):\n","                n_trims = len(select_idx) % int(EPOCH_SEC_SIZE * sampling_rate)\n","                n_label_trims = int(math.ceil(n_trims / (EPOCH_SEC_SIZE * sampling_rate)))\n","                select_idx = select_idx[:-n_trims]\n","                labels = labels[:-n_label_trims]\n","\n","        # removing all unknown and movement labels\n","        raw_ch = raw_ch_df.values[select_idx]\n","\n","        # check if we can split into epochs' size\n","        if len(raw_ch) % (EPOCH_SEC_SIZE * sampling_rate) != 0:\n","            raise Exception(\"Please choose anothe epoch duration!\")\n","        n_epochs = len(raw_ch) / (EPOCH_SEC_SIZE * sampling_rate)\n","\n","        # get epochs and their corresponding labels\n","        x = np.asarray(np.split(raw_ch, n_epochs)).astype(np.float32)\n","        y = labels.astype(np.int32)\n","\n","        assert len(x) == len(y)\n","\n","        # select on sleep periods\n","        w_edge_mins = 30\n","        nw_idx = np.where(y != label_dict[\"W\"])[0]\n","        start_idx = nw_idx[0] - (w_edge_mins * 2)\n","        end_idx = nw_idx[-1] + (w_edge_mins * 2)\n","        if start_idx < 0: start_idx = 0\n","        if end_idx >= len(y): end_idx = len(y) - 1\n","        select_idx = np.arange(start_idx, end_idx+1)\n","        x = x[select_idx]\n","        y = y[select_idx]\n","\n","        # file structure for saving\n","        save_dict = {\n","            \"x\": x, \n","            \"y\": y, \n","            \"fs\": sampling_rate,\n","            \"ch_label\": ch_labels,\n","            \"header_raw\": head_raw,\n","            \"header_annotation\": annot_raw,\n","        }\n","        if not os.path.exists(output_path):\n","            os.makedirs(output_path)\n","        np.savez(os.path.join(output_path, filename), **save_dict)\n"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09915fe4aeef4eaa92d00f037305822e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["e9113abfe1784ef58fb93cb2c3fa625b","b606df85aaaf41aca965d6f55cd000e6","c25a6e648f9b4826a168a7245c5bd485","a69e533d3699490f8d89f260d05bdbac","439efa5423d2461a89d2d32123532921","ae6e81444bdf447eac83fe661402a486","5a547df129984adb97c9677d28871306","273f243c216a44a3aab57358e265f3dc"]},"id":"Ka721CCocDEO","executionInfo":{"status":"ok","timestamp":1618885428377,"user_tz":240,"elapsed":171288,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}},"outputId":"0d5fe4e4-b874-425b-b3c2-833617ce4df0"},"source":["import glob\n","npz_files = sorted(glob.glob(os.path.join(output_path, \"*.npz\")))\n","X = np.zeros((0, 3000, 2)) # two channel 'EEG Fpz-Cz', 'EEG Pz-Oz'\n","y = []\n","for fn in tqdm(npz_files[:number_of_subj]):\n","    samples = np.load(fn)\n","    X_data = samples['x']\n","    X = np.concatenate((X, X_data), axis=0)\n","    y.extend(samples['y'])\n","y = np.array(y)"],"execution_count":14,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9113abfe1784ef58fb93cb2c3fa625b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"4YBatatxC0Ft","executionInfo":{"status":"ok","timestamp":1618885433649,"user_tz":240,"elapsed":837,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}},"outputId":"5f96d7df-a0aa-4935-a6c7-80d517ee1f52"},"source":["pd.Series(y).value_counts().plot.bar()\n","plt.title(\"Frequency of the labels in our dataset\")"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'Frequency of the labels in our dataset')"]},"metadata":{"tags":[]},"execution_count":15},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttdRNPedkoBT","executionInfo":{"status":"ok","timestamp":1618885445891,"user_tz":240,"elapsed":4814,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}},"outputId":"795a21ba-b3b3-4e6c-ca90-4b7e29dc178a"},"source":["from sklearn.model_selection import train_test_split\n","if VBS:\n","    print(\"Shape of the input data: {}\".format(X.shape))\n","    print(\"Shape of the sleep stages: {}\".format(y.shape))\n","# splitting subjects\n","X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.1, random_state=seed)\n","X_train_fpz_cz, X_test_fpz_cz, Y_train_fpz_cz, Y_test_fpz_cz = train_test_split(X[:,:,0:1], y, test_size=0.1, random_state=seed)\n","X_train_pz_oz, X_test_pz_oz, Y_train_pz_oz, Y_test_pz_oz = train_test_split(X[:,:,1:2], y, test_size=0.1, random_state=seed)\n","\n","if VBS:\n","    print(\"Shape of the training dataset:\\ntraining dataset: {}\\ntest dataset: {}\\n\"\n","          .format(X_train.shape, X_test.shape))\n","    print(\"Shape of the training dataset:\\ntraining fpz_cz dataset: {}\\ntest fpz_cz dataset: {}\\n\"\n","          .format(X_train_fpz_cz.shape, X_test_fpz_cz.shape))\n","    print(\"Shape of the training dataset:\\ntraining pz_oz dataset: {}\\ntest pz_oz dataset: {}\\n\"\n","          .format(X_train_pz_oz.shape, X_test_pz_oz.shape))\n","    \n","#print(y_train)\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Shape of the input data: (80462, 3000, 2)\n","Shape of the sleep stages: (80462,)\n","Shape of the training dataset:\n","training dataset: (72415, 3000, 2)\n","test dataset: (8047, 3000, 2)\n","\n","Shape of the training dataset:\n","training fpz_cz dataset: (72415, 3000, 1)\n","test fpz_cz dataset: (8047, 3000, 1)\n","\n","Shape of the training dataset:\n","training pz_oz dataset: (72415, 3000, 1)\n","test pz_oz dataset: (8047, 3000, 1)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yhZ5n2EznwRy","executionInfo":{"status":"ok","timestamp":1618848535594,"user_tz":240,"elapsed":81569,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}}},"source":["# !pip install git+https://www.github.com/keras-team/keras-contrib.git"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"5hoRr4pwmCDg","executionInfo":{"status":"ok","timestamp":1618848535596,"user_tz":240,"elapsed":81562,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}}},"source":["# from tensorflow import keras\n","# # from tensorflow.keras import optimizers, losses\n","# from tensorflow.keras.utils import to_categorical\n","\n","\n","# y_train_ = to_categorical(y_train)\n","# y_val_ = to_categorical(y_val)\n","# y_test_ = to_categorical(y_test)\n","#print(y_train)\n","\n","# from tensorflow.keras import optimizers, losses, activations, models\n","# from tensorflow.keras.models import Model, load_model\n","# from tensorflow.keras.layers import GlobalMaxPool1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D, TimeDistributed, Bidirectional, LSTM\n","# from tensorflow.keras.layers import Input, Conv1D, Dense, Dropout, MaxPool1D, Activation\n","# from tensorflow.keras.layers import Reshape, LSTM, TimeDistributed, Bidirectional, BatchNormalization, Flatten, RepeatVector\n","# from tensorflow.keras.optimizers import Adam\n","# from keras_contrib.layers import CRF\n","\n","# from scipy.signal import butter, lfilter\n","\n","# WINDOW_SIZE = 100\n","# Fs = 100\n","\n","# def rescale_array(X):\n","#     X = X / 20\n","#     X = np.clip(X, -5, 5)\n","#     return X\n","\n","# def aug_X(X):\n","#     scale = 1 + np.random.uniform(-0.1, 0.1)\n","#     offset = np.random.uniform(-0.1, 0.1)\n","#     noise = np.random.normal(scale=0.05, size=X.shape)\n","#     X = scale * X + offset + noise\n","#     return X\n","\n","# def gen(dict_files, scale=True, aug=False):\n","#     while True:\n","#         record_name = random.choice(list(dict_files.keys()))\n","#         batch_data = dict_files[record_name]\n","#         all_rows = batch_data['x']\n","\n","#         for i in range(10):\n","#             start_index = random.choice(range(all_rows.shape[0]-WINDOW_SIZE))\n","\n","#             X = all_rows[start_index:start_index+WINDOW_SIZE, ...]\n","#             Y = batch_data['y'][start_index:start_index+WINDOW_SIZE]\n","\n","#             X = np.expand_dims(X, 0)\n","#             Y = np.expand_dims(Y, -1)\n","#             Y = np.expand_dims(Y, 0)\n","\n","#             if aug:\n","#                 X = aug_X(X)\n","                \n","#             if scale:\n","#                 X = rescale_array(X)\n","\n","#             yield np.squeeze(X, axis=0), np.squeeze(Y, axis=0)\n","\n","# def data_generator(X_samples, y_samples, bs=32, label=True):\n","#     i = 0\n","# #     X_samples = X_samples[:]\n","# #     y_samples = y_samples[:]\n","#     while True:\n","#         X_temp = X_samples[i:i + bs]\n","#         y_temp = y_samples[i:i + bs]\n","        \n","#         X_temp = np.array([rescale_array(sample) for sample in X_temp])\n","#         y_temp = np.array(y_temp)\n","#         if label:\n","#             yield X_temp, y_temp\n","#         else:\n","#             yield X_temp\n","\n","#         i += bs           \n","        \n","\n","# def model_b(n_classes=5, use_sub_layer=False, use_rnn=True, verbose=False):\n","#     \"\"\"Recurrent_Deep_Neural_Networks_for_Real-Time_Sleep\n","#     \"\"\"\n","#     inputLayer = Input(shape=(3000, 1), name='inLayer')\n","#     convFine = Conv1D(filters=64, kernel_size=int(Fs/2), strides=int(Fs/16), padding='same', activation='relu', name='fConv1')(inputLayer)\n","#     convFine = MaxPool1D(pool_size=8, strides=8, name='fMaxP1')(convFine)\n","#     convFine = Dropout(rate=0.5, name='fDrop1')(convFine)\n","#     convFine = Conv1D(filters=128, kernel_size=8, padding='same', activation='relu', name='fConv2')(convFine)\n","#     convFine = Conv1D(filters=128, kernel_size=8, padding='same', activation='relu', name='fConv3')(convFine)\n","#     convFine = Conv1D(filters=128, kernel_size=8, padding='same', activation='relu', name='fConv4')(convFine)\n","#     convFine = MaxPool1D(pool_size=4, strides=4, name='fMaxP2')(convFine)\n","#     fineShape = convFine.get_shape()\n","#     convFine = Flatten(name='fFlat1')(convFine)\n","    \n","#     # network to learn coarse features\n","#     convCoarse = Conv1D(filters=32, kernel_size=Fs*4, strides=int(Fs/2), padding='same', activation='relu', name='cConv1')(inputLayer)\n","#     convCoarse = MaxPool1D(pool_size=4, strides=4, name='cMaxP1')(convCoarse)\n","#     convCoarse = Dropout(rate=0.5, name='cDrop1')(convCoarse)\n","#     convCoarse = Conv1D(filters=128, kernel_size=6, padding='same', activation='relu', name='cConv2')(convCoarse)\n","#     convCoarse = Conv1D(filters=128, kernel_size=6, padding='same', activation='relu', name='cConv3')(convCoarse)\n","#     convCoarse = Conv1D(filters=128, kernel_size=6, padding='same', activation='relu', name='cConv4')(convCoarse)\n","#     convCoarse = MaxPool1D(pool_size=2, strides=2, name='cMaxP2')(convCoarse)\n","#     coarseShape = convCoarse.get_shape()\n","#     convCoarse = Flatten(name='cFlat1')(convCoarse)\n","    \n","#     # concatenate coarse and fine cnns\n","#     mergeLayer = concatenate([convFine, convCoarse], name='merge_1')\n","#     outLayer = Dropout(rate=0.5, name='mDrop1')(mergeLayer)\n","    \n","#     outLayer = Reshape((1, outLayer.get_shape()[1]), name='reshape1')(outLayer)\n","#     outLayer = LSTM(64, return_sequences=True)(outLayer)\n","#     outLayer = LSTM(64, return_sequences=False)(outLayer)\n","\n","#     # Classify\n","#     outLayer = Dense(n_classes, activation='softmax', name='outLayer')(outLayer)\n","#     model = Model(inputLayer, outLayer)\n","#     optimizer = Adam(lr=1e-4)\n","#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n","#     if verbose:\n","#         model.summary()\n","#     return model\n","\n","# def evaluate_metrics(cm,classes):\n","\n","#     print (\"Confusion matrix:\")\n","#     print (cm)\n","\n","#     cm = cm.astype(np.float32)\n","#     FP = cm.sum(axis=0) - np.diag(cm)\n","#     FN = cm.sum(axis=1) - np.diag(cm)\n","#     TP = np.diag(cm)\n","#     TN = cm.sum() - (FP + FN + TP)\n","#     # https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n","#     # Sensitivity, hit rate, recall, or true positive rate\n","#     TPR = TP / (TP + FN)\n","#     # Specificity or true negative rate\n","#     TNR = TN / (TN + FP)\n","#     # Precision or positive predictive value\n","#     PPV = TP / (TP + FP)\n","#     # Negative predictive value\n","#     NPV = TN / (TN + FN)\n","#     # Fall out or false positive rate\n","#     FPR = FP / (FP + TN)\n","#     # False negative rate\n","#     FNR = FN / (TP + FN)\n","#     # False discovery rate\n","#     FDR = FP / (TP + FP)\n","\n","#     # Overall accuracy\n","#     ACC = (TP + TN) / (TP + FP + FN + TN)\n","#     # ACC_micro = (sum(TP) + sum(TN)) / (sum(TP) + sum(FP) + sum(FN) + sum(TN))\n","#     ACC_macro = np.mean(ACC) # to get a sense of effectiveness of our method on the small classes we computed this average (macro-average)\n","\n","#     F1 = (2 * PPV * TPR) / (PPV + TPR)\n","#     F1_macro = np.mean(F1)\n","\n","#     print (\"Sample: {}\".format(int(np.sum(cm))))\n","#     n_classes = len(classes)\n","#     for index_ in range(n_classes):\n","#         print (\"{}: {}\".format(classes[index_], int(TP[index_] + FN[index_])))\n","\n","\n","#     return ACC_macro,ACC, F1_macro, F1, TPR, TNR, PPV\n","    \n","# #train\n","# def butter_bandpass(lowcut, highpass, fs, order=4):\n","#     nyq = 0.5 * fs\n","#     #       low = lowcut / nyq\n","#     high = highpass / nyq\n","#     b, a = butter(order, high, btype='highpass')\n","#     return b, a\n","   \n","# def butter_bandpass_filter(data, highpass, fs, order=4):\n","#     b, a = butter_bandpass(0, highpass, fs, order=order)\n","#     y = lfilter(b, a, data)\n","#     return y "],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"l70D0imbmmQG","executionInfo":{"status":"ok","timestamp":1618848535598,"user_tz":240,"elapsed":81556,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}}},"source":["\n","# pp_X_train = np.array([butter_bandpass_filter(sample, highpass=40.0, fs=100, order=4) for sample in X_train])\n","# pp_X_val = np.array([butter_bandpass_filter(sample, highpass=40.0, fs=100, order=4) for sample in X_val])\n","# pp_X_test = np.array([butter_bandpass_filter(sample, highpass=40.0, fs=100, order=4) for sample in X_test])\n","# # pp_X_test = np.expand_dims(pp_X_test, axis=2)\n","# # pp_X_train = np.expand_dims(pp_X_train, axis=2)\n","# # pp_X_val = np.expand_dims(pp_X_val, axis=2)\n","# if VBS:\n","#     print(pp_X_val.shape)\n","#     print(pp_X_train.shape)\n","#     print(X_train.shape)\n","#     print(X_val.shape)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"_45KSu3HnJPW","executionInfo":{"status":"ok","timestamp":1618848535599,"user_tz":240,"elapsed":81548,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}}},"source":["# from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n","# checkpoint = ModelCheckpoint(\"model_cps\", monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n","# redonplat = ReduceLROnPlateau(monitor=\"val_loss\", mode=\"max\", patience=5, verbose=2)\n","# csv_logger = CSVLogger('log_training.csv', append=True, separator=',')\n","# callbacks_list = [\n","#     checkpoint,\n","#     redonplat,\n","#     csv_logger,\n","# ]"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"gvM37M-Jrsf2","executionInfo":{"status":"ok","timestamp":1618848535600,"user_tz":240,"elapsed":81541,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}}},"source":["# model_cnn = model_b(verbose=VBS)\n","# hist_19 = model_cnn.fit(\n","#     pp_X_train, y_train_, batch_size=batch_size, epochs=30, validation_data=(pp_X_val, y_val_), callbacks=callbacks_list, verbose=VBS\n","# )"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"lkgmbWiosHZK","executionInfo":{"status":"ok","timestamp":1618848537880,"user_tz":240,"elapsed":83812,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}}},"source":["from torch.utils.data import Dataset\n","class EEGDataset(Dataset):\n","\n","    def __init__(self, a):\n","        \"\"\"\n","        TODO: init the Dataset instance.\n","        \"\"\"\n","        self.X = a[0]\n","        self.Y = a[1]\n","\n","    def __len__(self):\n","        \"\"\"\n","        TODO: Denotes the total number of samples\n","        \"\"\"\n","\n","        return len(self.Y)\n","\n","    def __getitem__(self, i):\n","        return (self.X[i], self.Y[i])"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"viuAyGALGJOH","executionInfo":{"status":"ok","timestamp":1618848538083,"user_tz":240,"elapsed":84003,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# class MintNet(nn.Module):\n","#     def __init__(self, n=6):\n","#         \"\"\"\n","#         TODO : documents\n","#         \"\"\"\n","#         super(MintNet, self).__init__()\n","#         '''\n","#         Representation layer (initialization)\n","#         '''\n","#         Fs = 100\n","#         self.conv_1 = nn.Conv1d(in_channels=n, out_channels=64, kernel_size=int(Fs / 2), stride=int(Fs / 16), padding=int(Fs / 4) -1)\n","#         self.pool_1 = nn.MaxPool1d(kernel_size=8, stride=8)\n","#         self.droput_1 = nn.Dropout(p=.5)\n","#         self.conv_2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=8, padding=4)\n","#         self.conv_3 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=8, padding=4)\n","#         self.conv_4 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=8, padding=4)\n","#         self.pool_2 = nn.MaxPool1d(kernel_size=4, stride=4)\n","\n","#         '''\n","#         Representation layer (Fine-tuning)\n","#         '''\n","#         self.conv_1_ft = nn.Conv1d(in_channels=n, out_channels=32, kernel_size=int(Fs * 4), stride=int(Fs / 2), padding=int(Fs*2) -1)\n","#         self.pool_1_ft = nn.MaxPool1d(kernel_size=4, stride=4)\n","#         self.conv_2_ft = nn.Conv1d(in_channels=32, out_channels=128, kernel_size=6, padding=3)\n","#         self.conv_3_ft = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=6, padding=3)\n","#         self.conv_4_ft = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=6, padding=3)\n","#         self.pool_2_ft = nn.MaxPool1d(kernel_size=2, stride=2)\n","\n","#         '''\n","#         TODO - Some reshaping, We are not sure :)\n","#         '''\n","#         self.lstm_1 = nn.LSTM(input_size=1, hidden_size=512, num_layers=1, bidirectional=False)\n","#         self.lstm_2 = nn.LSTM(input_size=512, hidden_size=1, num_layers=1, bidirectional=False)\n","\n","#         '''\n","#         Fully Connected\n","#         '''\n","\n","#         self.fc = nn.Linear(in_features=3200, out_features=5)\n","\n","#     def forward(self, input, dev):\n","#         input = input.permute(0,2,1).to(dev)\n","#         #input = input.unsqueeze(dim=-1).permute(0,2,1).to(dev)\n","#         #print(input.shape)\n","#         x = self.conv_1(input)\n","#         x = F.relu(x)\n","#         x = self.pool_1(x)\n","#         x = self.droput_1(x)\n","#         x = F.relu(self.conv_2(x))\n","#         x = F.relu(self.conv_3(x))\n","#         x = F.relu(self.conv_4(x))\n","#         x = self.pool_2(x)\n","#         x = torch.flatten(x, start_dim=1)\n","\n","#         x_hat = self.conv_1_ft(input)\n","#         x_hat = F.relu(x_hat)\n","#         x_hat = self.pool_1_ft(x_hat)\n","#         x_hat = self.droput_1(x_hat)\n","#         x_hat = F.relu(self.conv_2_ft(x_hat))\n","#         x_hat = F.relu(self.conv_3_ft(x_hat))\n","#         x_hat = F.relu(self.conv_4_ft(x_hat))\n","#         x_hat = self.pool_2_ft(x_hat)\n","#         x_hat = torch.flatten(x_hat, start_dim=1)\n","#         merged_layers = torch.cat((x, x_hat), dim=-1)\n","#         out_hat = self.droput_1(merged_layers)\n","#         '''\n","#         TODO some reshaping required\n","#         '''\n","\n","#         out = out_hat.unsqueeze(dim=-1)\n","#         out, hidden = self.lstm_1(out)\n","#         out = self.droput_1(out)\n","#         out, hidden = self.lstm_2(out)\n","#         out = self.droput_1(out)\n","\n","#         # fc_input = out_hat.unsqueeze(dim=-1)\n","#         #\n","#         # out_hat = self.fc(fc_input)#correct this out must be from after merged_layer\n","#         #\n","#         # merged_layers = torch.cat((out, out_hat), dim=-1)\n","      \n","#         out = self.fc(out.squeeze(dim=-1))\n","#         out = self.droput_1(out)\n","#         #out = F.softmax(out, dim=1)\n","\n","#         return out"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"66HzLjubFHM7","executionInfo":{"status":"ok","timestamp":1618848545310,"user_tz":240,"elapsed":91221,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","BATCH_SIZE = 50\n","# train & test data from fpz-cz and pz_oz\n","X_train = torch.tensor(X_train).float().detach().clone()\n","Y_train = torch.tensor(Y_train).long().detach().clone()\n","train_data_all = (X_train, Y_train)\n","\n","X_test = torch.tensor(X_test).float().detach().clone()\n","Y_test = torch.tensor(Y_test).long().detach().clone()\n","test_data_all = (X_test, Y_test)\n","\n","train_loader_all = torch.utils.data.DataLoader(EEGDataset(train_data_all), batch_size=BATCH_SIZE, shuffle=False)\n","test_loader_all =  torch.utils.data.DataLoader(EEGDataset(test_data_all), batch_size=BATCH_SIZE, shuffle=False)\n","\n","# train & test data from fpz-cz\n","X_train_fpz_cz = torch.tensor(X_train_fpz_cz).float().detach().clone()\n","Y_train_fpz_cz = torch.tensor(Y_train_fpz_cz).long().detach().clone()\n","train_data_fpz_cz = (X_train_fpz_cz, Y_train_fpz_cz)\n","\n","X_test_fpz_cz = torch.tensor(X_test_fpz_cz).float().detach().clone()\n","Y_test_fpz_cz = torch.tensor(Y_test_fpz_cz).long().detach().clone()\n","test_data_fpz_cz = (X_test_fpz_cz, Y_test_fpz_cz)\n","\n","train_loader_fpz_cz = torch.utils.data.DataLoader(EEGDataset(train_data_fpz_cz), batch_size=BATCH_SIZE, shuffle=False)\n","test_loader_fpz_cz =  torch.utils.data.DataLoader(EEGDataset(test_data_fpz_cz), batch_size=BATCH_SIZE, shuffle=False)\n","\n","# train & test data from pz-oz\n","X_train_pz_oz = torch.tensor(X_train_pz_oz).float().detach().clone()\n","Y_train_pz_oz = torch.tensor(Y_train_pz_oz).long().detach().clone()\n","train_data_pz_oz = (X_train_pz_oz, Y_train_pz_oz)\n","\n","X_test_pz_oz= torch.tensor(X_test_pz_oz).float().detach().clone()\n","Y_test_pz_oz = torch.tensor(Y_test_pz_oz).long().detach().clone()\n","test_data_pz_oz = (X_test_pz_oz, Y_test_pz_oz)\n","\n","train_loader_pz_oz = torch.utils.data.DataLoader(EEGDataset(train_data_pz_oz), batch_size=BATCH_SIZE, shuffle=False)\n","test_loader_pz_oz =  torch.utils.data.DataLoader(EEGDataset(test_data_pz_oz), batch_size=BATCH_SIZE, shuffle=False)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gk3QyVPKOPVD","executionInfo":{"status":"ok","timestamp":1618848545316,"user_tz":240,"elapsed":91215,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}}},"source":["\n","def calculate_batch_accuracy_precision_recall_fscore(y_pred, y_true):\n","    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","    y_pred = torch.argmax(y_pred, dim=-1)\n","    y_pred = y_pred.cpu().data.numpy()\n","    y_true = y_true.cpu().data.numpy()\n","    (precision, recall, fscore, _) = precision_recall_fscore_support(y_true, y_pred, average='macro',warn_for=tuple())\n","    accuracy = accuracy_score(y_pred, y_true)\n","    #print(precision, recall, fscore)\n","    return precision, recall, fscore, accuracy\n","\n","def train_model(model, train_loader, n_epoch=5, lr=0.003, device=None):\n","    import torch.optim as optim\n","    \"\"\"\n","    Comments goes here\n","    \"\"\"\n","    device = device or torch.device('cpu')\n","   \n","    loss_history = []\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","    model.train()\n","    epoch_precision_hist, epoch_recall_hist, epoch_fscore_hist, epoch_accuracy_hist = ([] for i in range(4))\n","    for epoch in range(n_epoch):\n","        curr_epoch_loss = []\n","        precision, recall, fscore, accuracy = ([] for i in range(4))\n","        count = 0\n","        for X, Y in train_loader:\n","            optimizer.zero_grad()\n","            #X = X.squeeze(dim=0).to(device)\n","            X = X.detach().clone().to(device)\n","            Y = Y.detach().clone().to(device)\n","            Y_hat = model(X, device).float()\n","            Y = Y.long()\n","            loss = criterion(Y_hat, Y)\n","            loss.backward()\n","            optimizer.step()     \n","            # Appending loss     \n","            curr_epoch_loss.append(loss.cpu().data.numpy())\n","            # measure all metrics\n","            batch_precision, batch_recall, batch_fscore, batch_acc = calculate_batch_accuracy_precision_recall_fscore(Y_hat, Y)\n","            accuracy.append(batch_acc)\n","            fscore.append(batch_fscore)\n","            recall.append(batch_recall)\n","            precision.append(batch_precision)\n","            count+=1\n","            #print(count,\"/\",len(train_loader),\"---lr->\",lr,\"---------acc->\", batch_acc, \"  --- loss ->\", loss)\n","            print(f\"\\repoch{epoch} | progress->{count}/{len(train_loader)} \", end=\"\")\n","        epoch_accuracy = np.mean(accuracy)\n","        epoch_accuracy_hist.append(epoch_accuracy)\n","        epoch_precision_hist.append(np.mean(precision))\n","        epoch_recall_hist.append(np.mean(recall))\n","        epoch_fscore_hist.append(np.mean(fscore))\n","        print(f\"epoch{epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n","        loss_history.append(np.mean(curr_epoch_loss))\n","        print(datetime.now().strftime(\"%Y-%m-%d %H:%M\"), \"epoch_accuracy->\", epoch_accuracy)\n","        exp_lr_scheduler.step()\n","        ## Evaluating on every epoch\n","        # pred, truth = eval_model(model, val_loader, device=device)\n","        # auroc, f1 = evaluate_predictions(truth, pred)\n","        # print(f\"AUROC={auroc} and F1={f1}\")\n","    return model, loss_history, epoch_accuracy_hist, epoch_precision_hist, epoch_recall_hist, epoch_fscore_hist"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"DO-t7obEDsDT","executionInfo":{"status":"ok","timestamp":1618848545317,"user_tz":240,"elapsed":91207,"user":{"displayName":"Sanjay Kumar","photoUrl":"","userId":"11166364377275833862"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SleepCNN_1D_Tsinalis(nn.Module):\n","    \"\"\"Replication of the CNN architecture used by Tsinalis,\n","    et al. (2016)\n","    https://arxiv.org/pdf/1610.01683.pdf\n","    \"\"\"\n","\n","    def __init__(self, n = 1):\n","        super(SleepCNN_1D_Tsinalis, self).__init__()\n","        self.conv1 = nn.Conv1d(in_channels=n, out_channels=20, kernel_size=200, stride=1)\n","        self.pool1 = nn.MaxPool1d(kernel_size=20, stride=10)\n","        self.conv2 = nn.Conv2d(\n","            in_channels=1, out_channels=400, kernel_size=(20, 30), stride=(1, 1))\n","        self.pool2 = nn.MaxPool2d(kernel_size=(1, 10), stride=(1, 2))\n","        #self.fc1 = nn.Linear(in_features=400 * 721, out_features=500)\n","        self.fc1 = nn.Linear(in_features=400 * 121, out_features=500)\n","        self.fc2 = nn.Linear(in_features=500, out_features=5)\n","\n","    def forward(self, x, dev):\n","        x = x.permute(0,2,1).to(dev)\n","        # One dimensional convolution/pooling\n","        x = F.relu(self.conv1(x))\n","        x = self.pool1(x)\n","\n","        # Stack, two-dimensional convolution/pooling\n","        x = x.unsqueeze(1)\n","        x = F.relu(self.conv2(x))\n","        x = self.pool2(x)\n","        #print(x.shape)\n","        # Reshape, fully connected layers\n","        x = x.view(-1, 400 * 121)#400 * 721)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","\n","        return x"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tLtXgaV9IUdE","outputId":"935b0c72-8cfb-464a-a92c-99edb4c009aa"},"source":["print(torch.cuda.is_available())\n","if torch.cuda.is_available():\n","  dev = \"cuda:0\"\n","  torch.cuda.empty_cache()\n","else:\n","  dev = \"cpu\"\n","device = torch.device(dev)\n","print(datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n","\n","# All the channel\n","n_epoch = 20#10\n","lr = .0001#0.003\n","\n","n_dim=2#number of channels which includes fpz-cz and pz-oz\n","\n","model = SleepCNN_1D_Tsinalis(n_dim)#MintNet(n_dim)\n","model = model.to(device)\n","\n","model, loss_history, epoch_accuracy_hist, epoch_precision_hist, epoch_recall_hist, epoch_fscore_hist = train_model(model, train_loader_all, n_epoch=n_epoch, lr=lr, device=device)\n","torch.save(model, os.path.join(project_path, \"SleepCNN_multichannel.pth\"))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["False\n","2021-04-19 16:09\n","epoch0 | progress->1008/1008 epoch0: curr_epoch_loss=1.1108107566833496\n","2021-04-19 16:25 epoch_accuracy-> 0.5478968253968254\n","epoch1 | progress->1008/1008 epoch1: curr_epoch_loss=0.767231822013855\n","2021-04-19 16:42 epoch_accuracy-> 0.7044692460317461\n","epoch2 | progress->1008/1008 epoch2: curr_epoch_loss=0.6883872747421265\n","2021-04-19 16:59 epoch_accuracy-> 0.7390029761904762\n","epoch3 | progress->1008/1008 epoch3: curr_epoch_loss=0.6275800466537476\n","2021-04-19 17:17 epoch_accuracy-> 0.7679464285714285\n","epoch4 | progress->1008/1008 epoch4: curr_epoch_loss=0.6188586950302124\n","2021-04-19 17:34 epoch_accuracy-> 0.7714236111111111\n","epoch5 | progress->1008/1008 epoch5: curr_epoch_loss=0.6125154495239258\n","2021-04-19 17:51 epoch_accuracy-> 0.7737450396825396\n","epoch6 | progress->1008/1008 epoch6: curr_epoch_loss=0.6046729683876038\n","2021-04-19 18:08 epoch_accuracy-> 0.7776984126984128\n","epoch7 | progress->1008/1008 epoch7: curr_epoch_loss=0.6037918329238892\n","2021-04-19 18:26 epoch_accuracy-> 0.7775992063492063\n","epoch8 | progress->1008/1008 epoch8: curr_epoch_loss=0.603135883808136\n","2021-04-19 18:43 epoch_accuracy-> 0.7778968253968255\n","epoch9 | progress->1008/1008 epoch9: curr_epoch_loss=0.6021963357925415\n","2021-04-19 19:01 epoch_accuracy-> 0.7773462301587302\n","epoch10 | progress->621/1008 "],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8vhVoS3cXwzl"},"source":["#fpz-cz training\n","print('fpz-cz training', datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n","n_epoch = 20#10\n","lr = .0001#0.003\n","\n","n_dim=1#number of channels as its fpz-cz\n","\n","model_fpz_cz = SleepCNN_1D_Tsinalis(n_dim)#MintNet(n_dim)\n","model_fpz_cz = model_fpz_cz.to(device)\n","\n","model_fpz_cz, loss_history_fpz_cz, epoch_accuracy_hist_fpz_cz, epoch_precision_hist_fpz_cz, epoch_recall_hist_fpz_cz, epoch_fscore_hist_fpz_cz = train_model(model_fpz_cz, train_loader_fpz_cz, n_epoch=n_epoch, lr=lr, device=device)\n","torch.save(model_fpz_cz, os.path.join(project_path, \"SleepCNN_fpz_cz_channel.pth\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bSbz2JXXXyDG"},"source":["#pz-oz training\n","print('pz-oz training', datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n","n_epoch = 20#10\n","lr = .0001#0.003\n","\n","n_dim=1#number of channels as its pz-oz\n","\n","model_pz_oz = SleepCNN_1D_Tsinalis(n_dim)#MintNet(n_dim)\n","model_pz_oz = model_pz_oz.to(device)\n","\n","model_pz_oz, loss_history_pz_oz, epoch_accuracy_hist_pz_oz, epoch_precision_hist_pz_oz, epoch_recall_hist_pz_oz, epoch_fscore_hist_pz_oz = train_model(model_pz_oz, train_loader_pz_oz, n_epoch=n_epoch, lr=lr, device=device)\n","torch.save(model_pz_oz, os.path.join(project_path, \"SleepCNN_pz_oz_channel.pth\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gGYrgYXK2JB3"},"source":["plt.plot(loss_history)\n","plt.plot(loss_history_fpz_cz)\n","plt.plot(loss_history_pz_oz)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sjpfCICLhO0q"},"source":["plt.plot(epoch_accuracy_hist)\n","plt.plot(epoch_accuracy_hist_fpz_cz)\n","plt.plot(epoch_accuracy_hist_pz_oz)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQw3We83X01m"},"source":["plt.plot(epoch_precision_hist)\n","plt.plot(epoch_precision_hist_fpz_cz)\n","plt.plot(epoch_precision_hist_pz_oz)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-SycgLV-hcdt"},"source":["plt.plot(epoch_recall_hist)\n","plt.plot(epoch_recall_hist_fpz_cz)\n","plt.plot(epoch_recall_hist_pz_oz)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zNEYL1DUhkpl"},"source":["plt.plot(epoch_fscore_hist)\n","plt.plot(epoch_fscore_hist_fpz_cz)\n","plt.plot(epoch_fscore_hist_pz_oz)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cCX3eidJKWqa"},"source":["def eval_model(model, dataloader, device=None):\n","    \"\"\"\n","    Comments goes here\n","    \"\"\"\n","    device = device or torch.device('cpu')\n","    model.eval()\n","    pred_all = []\n","    Y_test = []\n","    for X, Y in dataloader:\n","        X = X.detach().clone().to(device)\n","        Y = Y.detach().clone().to(device)\n","        Y_hat = model(X, device).float()\n","        Y_hat = torch.softmax(Y_hat, dim=1)\n","        pred_all.append(Y_hat.cpu().detach().numpy())\n","        Y_test.append(Y.cpu().detach().numpy())\n","    pred_all = np.concatenate(pred_all, axis=0)\n","    Y_test = np.concatenate(Y_test, axis=0)\n","\n","    return pred_all, Y_test     \n","\n","def evaluate_predictions(truth, pred):\n","\n","    from sklearn.metrics import roc_auc_score, f1_score, cohen_kappa_score\n","  \n","    pred_hat = np.argmax(pred, axis=1)\n","\n","    auroc = roc_auc_score(truth, pred, multi_class='ovr')\n","    f1 = f1_score(truth, pred_hat, average='micro')\n","    kappa = cohen_kappa_score(truth, pred_hat)\n","\n","    return auroc, f1, kappa \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GtKQxweGEAf"},"source":["print(\"--------final Eval---of all Channels-----\")\n","pred, truth = eval_model(model, test_loader_all, device=device)\n","auroc, f1, kappa = evaluate_predictions(truth, pred)\n","print(f\"all Channels AUROC={auroc} and F1={f1} and Cohen's Kappa ={kappa}\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"77cJSQAJmJt1"},"source":["print(\"--------final Eval---of fpz-cz-----\")\n","pred, truth = eval_model(model_fpz_cz, test_loader_fpz_cz, device=device)\n","auroc, f1, kappa = evaluate_predictions(truth, pred)\n","print(f\"fpz-cz channel AUROC={auroc} and F1={f1} and Cohen's Kappa ={kappa}\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XoE-ypckmOCe"},"source":["print(\"--------final Eval---of pz-oz-----\")\n","pred, truth = eval_model(model_pz_oz, test_loader_pz_oz, device=device)\n","auroc, f1, kappa = evaluate_predictions(truth, pred)\n","print(f\"pz-oz channel AUROC={auroc} and F1={f1} and Cohen's Kappa ={kappa}\")\n","\n","print(\"End of run-->\",datetime.now().strftime(\"%Y-%m-%d %H:%M\"))"],"execution_count":null,"outputs":[]}]}