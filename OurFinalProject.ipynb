{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import ntpath\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, IterableDataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mne.io import read_raw_edf\n",
    "\n",
    "import dhedfreader\n",
    "\n",
    "# Label values\n",
    "W = 0\n",
    "N1 = 1\n",
    "N2 = 2\n",
    "N3 = 3\n",
    "REM = 4\n",
    "UNKNOWN = 5\n",
    "\n",
    "stage_dict = {\n",
    "    \"W\": W,\n",
    "    \"N1\": N1,\n",
    "    \"N2\": N2,\n",
    "    \"N3\": N3,\n",
    "    \"REM\": REM,\n",
    "    \"UNKNOWN\": UNKNOWN\n",
    "}\n",
    "\n",
    "class_dict = {\n",
    "    0: \"W\",\n",
    "    1: \"N1\",\n",
    "    2: \"N2\",\n",
    "    3: \"N3\",\n",
    "    4: \"REM\",\n",
    "    5: \"UNKNOWN\"\n",
    "}\n",
    "\n",
    "ann2label = {\n",
    "    \"Sleep stage W\": 0,\n",
    "    \"Sleep stage 1\": 1,\n",
    "    \"Sleep stage 2\": 2,\n",
    "    \"Sleep stage 3\": 3,\n",
    "    \"Sleep stage 4\": 3,\n",
    "    \"Sleep stage R\": 4,\n",
    "    \"Sleep stage ?\": 5,\n",
    "    \"Movement time\": 5\n",
    "}\n",
    "\n",
    "EPOCH_SEC_SIZE = 30\n",
    "#data_dir=\"/Users/pradeep/Desktop/mcs/cs598DLhealthcare/project/sleep-edf-database-expanded-1.0.0/sleep-cassette/\"\n",
    "data_dir = \"C:/Users/omesha/Documents/Illinois/sleepdata/Sleep-Stage-Classification-master/Sleep-Stage-Classification-master/physionet-sleep-data\"\n",
    "#output_dir=\"/Users/pradeep/Desktop/mcs/cs598DLhealthcare/project/EEGFPzCz/\"\n",
    "output_dir = data_dir + \"/out\"\n",
    "psg_fnames = glob.glob(os.path.join(data_dir, \"*PSG.edf\"))\n",
    "ann_fnames = glob.glob(os.path.join(data_dir, \"*Hypnogram.edf\"))\n",
    "psg_fnames.sort()\n",
    "ann_fnames.sort()\n",
    "psg_fnames = np.asarray(psg_fnames)\n",
    "ann_fnames = np.asarray(ann_fnames)\n",
    "##['EEG Fpz-Cz', 'EEG Pz-Oz', 'EOG horizontal', 'Resp oro-nasal', 'EMG submental', 'Temp rectal', 'Event marker']\n",
    "select_ch = 'EEG Fpz-Cz'\n",
    "\n",
    "\n",
    "##Below is working code corrected\n",
    "\n",
    "def getBatch(number_of_subj, output_dir):\n",
    "    npz_files = sorted(glob.glob(os.path.join(output_dir, \"*.npz\")))\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "    for fn in npz_files[:number_of_subj]:\n",
    "        samples = np.load(fn)\n",
    "        X_data.extend(samples['x'])\n",
    "        Y_data.extend(samples['y'])\n",
    "    return (X_data, Y_data)\n",
    "\n",
    "\n",
    "print(\"++++++++++++++\")\n",
    "\n",
    "##['EEG Fpz-Cz', 'EEG Pz-Oz', 'EOG horizontal', 'Resp oro-nasal', 'EMG submental', 'Temp rectal', 'Event marker']\n",
    "select_ch = ['EEG Fpz-Cz', 'EEG Pz-Oz', 'EOG horizontal', 'Resp oro-nasal', 'EMG submental', 'Temp rectal']\n",
    "#output_dirnew=\"/Users/pradeep/Desktop/mcs/cs598DLhealthcare/project/EEGFPzCz/test/\"\n",
    "output_dirnew = data_dir + \"/out/test\"\n",
    "#####Multichannel code\n",
    "for i in range(len(psg_fnames)):\n",
    "    if not \"SC4001E0-PSG.edf\" in psg_fnames[i]:\n",
    "        continue\n",
    "    # existfileorNot = ntpath.basename(psg_fnames[i]).replace(\"-PSG.edf\", \".npz\")\n",
    "    # if(os.path.exists(os.path.join(output_dirnew, existfileorNot))):\n",
    "    #     continue\n",
    "\n",
    "    raw = read_raw_edf(psg_fnames[i], preload=True, stim_channel=None)\n",
    "    sampling_rate = raw.info['sfreq']\n",
    "    raw_ch_df = raw.to_data_frame(scalings=100)[select_ch]\n",
    "    #raw_ch_df = raw_ch_df.to_frame()\n",
    "    raw_ch_df.set_index(np.arange(len(raw_ch_df)))\n",
    "\n",
    "    # Get raw header\n",
    "    f = open(psg_fnames[i], 'r', encoding='iso-8859-1')\n",
    "    reader_raw = dhedfreader.BaseEDFReader(f)\n",
    "    reader_raw.read_header()\n",
    "    h_raw = reader_raw.header\n",
    "\n",
    "    f.close()\n",
    "    raw_start_dt = datetime.strptime(h_raw['date_time'], \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Read annotation and its header\n",
    "    f = open(ann_fnames[i], 'r', encoding='iso-8859-1')\n",
    "    reader_ann = dhedfreader.BaseEDFReader(f)\n",
    "    reader_ann.read_header()\n",
    "    h_ann = reader_ann.header\n",
    "    _, _, ann = list(zip(*reader_ann.records()))\n",
    "    f.close()\n",
    "    ann_start_dt = datetime.strptime(h_ann['date_time'], \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Assert that raw and annotation files start at the same time\n",
    "    assert raw_start_dt == ann_start_dt\n",
    "\n",
    "    # Generate label and remove indices\n",
    "    remove_idx = []  # indicies of the data that will be removed\n",
    "    labels = []  # indicies of the data that have labels\n",
    "    label_idx = []\n",
    "    for a in ann[0]:\n",
    "        onset_sec, duration_sec, ann_char = a\n",
    "        ann_str = \"\".join(ann_char)\n",
    "        label = ann2label[ann_str]\n",
    "        if label != UNKNOWN:\n",
    "            if duration_sec % EPOCH_SEC_SIZE != 0:\n",
    "                raise Exception(\"Something wrong\")\n",
    "            duration_epoch = int(duration_sec / EPOCH_SEC_SIZE)\n",
    "            label_epoch = np.ones(duration_epoch, dtype=np.int) * label\n",
    "            labels.append(label_epoch)\n",
    "            idx = int(onset_sec * sampling_rate) + np.arange(duration_sec * sampling_rate, dtype=np.int)\n",
    "            label_idx.append(idx)\n",
    "\n",
    "            print(\"Include onset:{}, duration:{}, label:{} ({})\".format(\n",
    "                onset_sec, duration_sec, label, ann_str\n",
    "            ))\n",
    "        else:\n",
    "            idx = int(onset_sec * sampling_rate) + np.arange(duration_sec * sampling_rate, dtype=np.int)\n",
    "            remove_idx.append(idx)\n",
    "\n",
    "            print(\"Remove onset:{}, duration:{}, label:{} ({})\".format(\n",
    "                onset_sec, duration_sec, label, ann_str\n",
    "            ))\n",
    "    labels = np.hstack(labels)\n",
    "\n",
    "    print(\"before remove unwanted: {}\".format(np.arange(len(raw_ch_df)).shape))\n",
    "    if len(remove_idx) > 0:\n",
    "        remove_idx = np.hstack(remove_idx)\n",
    "        select_idx = np.setdiff1d(np.arange(len(raw_ch_df)), remove_idx)\n",
    "    else:\n",
    "        select_idx = np.arange(len(raw_ch_df))\n",
    "    print(\"after remove unwanted: {}\".format(select_idx.shape))\n",
    "\n",
    "    # Select only the data with labels\n",
    "    print(\"before intersect label: {}\".format(select_idx.shape))\n",
    "    label_idx = np.hstack(label_idx)\n",
    "    select_idx = np.intersect1d(select_idx, label_idx)\n",
    "    print(\"after intersect label: {}\".format(select_idx.shape))\n",
    "\n",
    "    # Remove extra index\n",
    "    if len(label_idx) > len(select_idx):\n",
    "        print(\"before remove extra labels: {}, {}\".format(select_idx.shape, labels.shape))\n",
    "        extra_idx = np.setdiff1d(label_idx, select_idx)\n",
    "        # Trim the tail\n",
    "        if np.all(extra_idx > select_idx[-1]):\n",
    "            n_trims = len(select_idx) % int(EPOCH_SEC_SIZE * sampling_rate)\n",
    "            n_label_trims = int(math.ceil(n_trims / (EPOCH_SEC_SIZE * sampling_rate)))\n",
    "            select_idx = select_idx[:-n_trims]\n",
    "            labels = labels[:-n_label_trims]\n",
    "        print(\"after remove extra labels: {}, {}\".format(select_idx.shape, labels.shape))\n",
    "\n",
    "    if (select_idx.shape[0] == 0):\n",
    "        continue\n",
    "    # Remove movement and unknown stages if any\n",
    "    raw_ch = raw_ch_df.values[select_idx]\n",
    "\n",
    "    # Verify that we can split into 30-s epochs\n",
    "    if len(raw_ch) % (EPOCH_SEC_SIZE * sampling_rate) != 0:\n",
    "        raise Exception(\"Something wrong\")\n",
    "    n_epochs = len(raw_ch) / (EPOCH_SEC_SIZE * sampling_rate)\n",
    "\n",
    "    # Get epochs and their corresponding labels\n",
    "    x = np.asarray(np.split(raw_ch, n_epochs)).astype(np.float32)\n",
    "    y = labels.astype(np.int32)\n",
    "\n",
    "    assert len(x) == len(y)\n",
    "\n",
    "    # Select on sleep periods\n",
    "    w_edge_mins = 30\n",
    "    nw_idx = np.where(y != stage_dict[\"W\"])[0]\n",
    "    start_idx = nw_idx[0] - (w_edge_mins * 2)\n",
    "    end_idx = nw_idx[-1] + (w_edge_mins * 2)\n",
    "    if start_idx < 0: start_idx = 0\n",
    "    if end_idx >= len(y): end_idx = len(y) - 1\n",
    "    select_idx = np.arange(start_idx, end_idx + 1)\n",
    "    print((\"Data before selection: {}, {}\".format(x.shape, y.shape)))\n",
    "    x = x[select_idx]\n",
    "    y = y[select_idx]\n",
    "    print((\"Data after selection: {}, {}\".format(x.shape, y.shape)))\n",
    "\n",
    "    # Save\n",
    "    filename = ntpath.basename(psg_fnames[i]).replace(\"-PSG.edf\", \".npz\")\n",
    "    save_dict = {\n",
    "        \"x\": x,\n",
    "        \"y\": y,\n",
    "        \"fs\": sampling_rate,\n",
    "        \"ch_label\": select_ch,\n",
    "        \"header_raw\": h_raw,\n",
    "        \"header_annotation\": h_ann,\n",
    "    }\n",
    "    np.savez(os.path.join(output_dirnew, filename), **save_dict)\n",
    "\n",
    "    print(\"\\n=======================================\\n\")\n",
    "\n",
    "\n",
    "def getBatch(number_of_subj, output_dirnew):\n",
    "    npz_files = sorted(glob.glob(os.path.join(output_dirnew, \"*.npz\")))\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "    for fn in npz_files[:number_of_subj]:\n",
    "        samples = np.load(fn)\n",
    "        X_data.extend(samples['x'])\n",
    "        Y_data.extend(samples['y'])\n",
    "    return (X_data, Y_data)\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "\n",
    "    def __init__(self, a):\n",
    "        \"\"\"\n",
    "        TODO: init the Dataset instance.\n",
    "        \"\"\"\n",
    "        self.X = a[0]\n",
    "        self.Y = a[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        TODO: Denotes the total number of samples\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.X[i], self.Y[i])\n",
    "\n",
    "\n",
    "def load_data(dataset, batch_size=100):\n",
    "    \"\"\"\n",
    "    Return a DataLoader instance basing on a Dataset instance, with batch_size specified.\n",
    "    Note that since the data has already been shuffled, we set shuffle=False\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "train_loader = load_data(EEGDataset(getBatch(5, output_dirnew)))\n",
    "for a, b in train_loader:\n",
    "    print(a.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MintNet(nn.Module):\n",
    "    def __init__(self, n_channels=1, n=3000):\n",
    "        \"\"\"\n",
    "        TODO : documents\n",
    "        \"\"\"        \n",
    "        super(MintNet, self).__init__()\n",
    "        '''\n",
    "        Representation layer (initialization)\n",
    "        \n",
    "        '''\n",
    "        Fs = 100\n",
    "        self.conv_1 = nn.Conv1d(in_channels=n,out_channels=64,kernel_size=int(Fs/2),stride=int(Fs/16))\n",
    "        self.pool_1 = nn.MaxPool1d(kernel_size=8, stride=8)\n",
    "        self.droput_1 = nn.Dropout(p=.5)\n",
    "        self.conv_2 = nn.Conv1d(in_channels=64,out_channels=128,kernel_size=8)\n",
    "        self.conv_3 = nn.Conv1d(in_channels=64,out_channels=128,kernel_size=8)\n",
    "        self.conv_4 = nn.Conv1d(in_channels=64,out_channels=128,kernel_size=8)\n",
    "        self.pool_2 = nn.MaxPool1d(kernel_size=8,stride=4)\n",
    "        \n",
    "        '''\n",
    "        Representation layer (Fine-tuning)\n",
    "        \n",
    "        '''\n",
    "        self.conv_1_ft = nn.Conv1d(in_channels=n,out_channels=32,kernel_size=int(Fs*4),stride=int(Fs*2))\n",
    "        self.pool_1_ft = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        self.conv_2_ft = nn.Conv1d(in_channels=32,out_channels=128,kernel_size=6)\n",
    "        self.conv_3_ft = nn.Conv1d(in_channels=32,out_channels=128,kernel_size=6)\n",
    "        self.conv_4_ft = nn.Conv1d(in_channels=32,out_channels=128,kernel_size=6)\n",
    "        self.pool_2_ft = nn.MaxPool1d(kernel_size=2,stride=2)\n",
    "        \n",
    "        '''\n",
    "        TODO - Some reshaping, We are not sure :)\n",
    "        '''\n",
    "        self.lstm = nn.LSTM(input_size = 512, hidden_size = 512, num_layers = 1, batch_first = True, bidirectional = True)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Fully Connected\n",
    "        '''\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=1024, out_features= 1)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        x = self.conv_1(input)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool_1(x)\n",
    "        x = self.droput_1(x)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = F.relu(self.conv_3(x))\n",
    "        x = F.relu(self.conv_4(x))\n",
    "        x = self.pool_2(x)\n",
    "        x = torch.flatten(x)\n",
    "        \n",
    "        x_hat = self.conv_1_ft(input)\n",
    "        x_hat = F.relu(x_hat)\n",
    "        x_hat = self.pool_1_ft(x_hat)\n",
    "        x_hat = self.droput_1(x_hat)\n",
    "        x_hat = F.relu(self.conv_2_ft(x_hat))\n",
    "        x_hat = F.relu(self.conv_3_ft(x_hat))\n",
    "        x_hat = F.relu(self.conv_4_ft(x_hat))\n",
    "        x_hat = self.pool_2_ft(x_hat)\n",
    "        x_hat = torch.flatten(x_hat)\n",
    "        \n",
    "        merged_layers = torch.cat((x, x_hat), dim =-1)\n",
    "        out = self.droput_1(merged_layers)\n",
    "        '''\n",
    "        TODO some reshaping required\n",
    "        '''\n",
    "        out = self.lstm(out)\n",
    "        out = self.droput_1(out)\n",
    "        out = self.lstm(out)\n",
    "        out = self.droput_1(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.droput_1(out)\n",
    "        out = F.softmax(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "model = MintNet()\n",
    "\n",
    "print(model)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, n_epoch=5, lr=0.003, device=None):\n",
    "    import torch.optim as optim\n",
    "    \"\"\"\n",
    "    Comments goes here\n",
    "    \"\"\"\n",
    "    device = device or torch.device('cpu')\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        curr_epoch_loss = []\n",
    "        for (X, Y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            Y_hat,_ = model(X)  \n",
    "            loss = criterion(Y_hat, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        print(f\"epoch{epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "        loss_history += curr_epoch_loss\n",
    "    return model, loss_history\n",
    "\n",
    "def eval_model(model, dataloader, device=None):\n",
    "    \"\"\"\n",
    "    Comments goes here\n",
    "    \"\"\"\n",
    "    device = device or torch.device('cpu')\n",
    "    model.eval()\n",
    "    pred_all = []\n",
    "    Y_test = []\n",
    "    for (X, K_beat, K_rhythm, K_freq), Y in dataloader:\n",
    "        Y_hat,_ = model(X, K_beat, K_rhythm, K_freq)\n",
    "        pred_all.append(Y_hat.detach().numpy())\n",
    "        Y_test.append(Y.detach().numpy())\n",
    "    pred_all = np.concatenate(pred_all, axis=0)\n",
    "    Y_test = np.concatenate(Y_test, axis=0)\n",
    "\n",
    "    return pred_all, Y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "n_epoch = 4\n",
    "lr = 0.003\n",
    "\n",
    "n_dim=3000\n",
    "\n",
    "\n",
    "model = MintNet(n_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "model, loss_history = train_model(model, train_loader, n_epoch=n_epoch, lr=lr, device=device)\n",
    "pred, truth = eval_model(model, test_loader, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DM_Final_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}